{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper methods\n",
    "\n",
    "class DeepLabModel(object):\n",
    "  \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
    "\n",
    "  INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
    "  OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
    "  INPUT_SIZE = 513\n",
    "  FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "\n",
    "  def __init__(self, modelPath, isTarFile=False):\n",
    "    \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
    "    self.graph = tf.Graph()\n",
    "\n",
    "    graph_def = None\n",
    "    if isTarFile:\n",
    "    # Extract frozen graph from tar archive.\n",
    "      tar_file = tarfile.open(modelPath)\n",
    "      for tar_info in tar_file.getmembers():\n",
    "        if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "          file_handle = tar_file.extractfile(tar_info)\n",
    "          graph_def = tf.GraphDef.FromString(file_handle.read())\n",
    "          break\n",
    "\n",
    "      tar_file.close()\n",
    "    else:\n",
    "      with open(modelPath, 'rb') as file_handle:\n",
    "        graph_def = tf.GraphDef.FromString(file_handle.read())\n",
    "\n",
    "\n",
    "    if graph_def is None:\n",
    "      raise RuntimeError('Cannot find inference graph in tar archive.')\n",
    "\n",
    "    with self.graph.as_default():\n",
    "      tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    self.sess = tf.Session(graph=self.graph)\n",
    "\n",
    "  def run(self, image):\n",
    "    \"\"\"Runs inference on a single image.\n",
    "\n",
    "    Args:\n",
    "      image: A PIL.Image object, raw input image.\n",
    "\n",
    "    Returns:\n",
    "      resized_image: RGB image resized from original input image.\n",
    "      seg_map: Segmentation map of `resized_image`.\n",
    "    \"\"\"\n",
    "    width, height = image.size\n",
    "    resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
    "    target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "    resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
    "    batch_seg_map = self.sess.run(\n",
    "        self.OUTPUT_TENSOR_NAME,\n",
    "        feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
    "    seg_map = batch_seg_map[0]\n",
    "    return resized_image, seg_map\n",
    "\n",
    "\n",
    "def create_pascal_label_colormap():\n",
    "  \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n",
    "\n",
    "  Returns:\n",
    "    A Colormap for visualizing segmentation results.\n",
    "  \"\"\"\n",
    "  colormap = np.zeros((256, 3), dtype=int)\n",
    "  ind = np.arange(256, dtype=int)\n",
    "\n",
    "  for shift in reversed(range(8)):\n",
    "    for channel in range(3):\n",
    "      colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
    "    ind >>= 3\n",
    "\n",
    "  return colormap\n",
    "\n",
    "\n",
    "def label_to_color_image(label):\n",
    "  \"\"\"Adds color defined by the dataset colormap to the label.\n",
    "\n",
    "  Args:\n",
    "    label: A 2D array with integer type, storing the segmentation label.\n",
    "\n",
    "  Returns:\n",
    "    result: A 2D array with floating type. The element of the array\n",
    "      is the color indexed by the corresponding element in the input label\n",
    "      to the PASCAL color map.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If label is not of rank 2 or its value is larger than color\n",
    "      map maximum entry.\n",
    "  \"\"\"\n",
    "  if label.ndim != 2:\n",
    "    raise ValueError('Expect 2-D input label')\n",
    "\n",
    "  colormap = create_pascal_label_colormap()\n",
    "\n",
    "  if np.max(label) >= len(colormap):\n",
    "    raise ValueError('label value too large.')\n",
    "\n",
    "  return colormap[label]\n",
    "\n",
    "\n",
    "def vis_segmentation(image, seg_map):\n",
    "  \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n",
    "  plt.figure(figsize=(15, 5))\n",
    "  grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
    "\n",
    "  plt.subplot(grid_spec[0])\n",
    "  plt.imshow(image)\n",
    "  plt.axis('off')\n",
    "  plt.title('input image')\n",
    "\n",
    "  plt.subplot(grid_spec[1])\n",
    "  seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
    "  plt.imshow(seg_image)\n",
    "  plt.axis('off')\n",
    "  plt.title('segmentation map')\n",
    "\n",
    "  plt.subplot(grid_spec[2])\n",
    "  plt.imshow(image)\n",
    "  plt.imshow(seg_image, alpha=0.7)\n",
    "  plt.axis('off')\n",
    "  plt.title('segmentation overlay')\n",
    "\n",
    "  unique_labels = np.unique(seg_map)\n",
    "  ax = plt.subplot(grid_spec[3])\n",
    "  plt.imshow(\n",
    "      FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
    "  ax.yaxis.tick_right()\n",
    "  plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
    "  plt.xticks([], [])\n",
    "  ax.tick_params(width=0.0)\n",
    "  plt.grid('off')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "LABEL_NAMES = np.asarray([\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "])\n",
    "\n",
    "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
    "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/AT073_10_Orig_Style/Libs/Tensorflow/models/data/crown_du-2019-01-09'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jovyan/AT073_10_Orig_Style/Libs/Tensorflow/models/data/deejaysoda-2018-12-08/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>file</th>\n",
       "      <th>doc_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43766464_257490018303217_3555742597326072823_n...</td>\n",
       "      <td>/home/jovyan/AT073_10_Orig_Style/Libs/Tensorfl...</td>\n",
       "      <td>43766464_257490018303217_3555742597326072823_n</td>\n",
       "      <td>json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37812209_310102473067178_1157119865203458048_n...</td>\n",
       "      <td>/home/jovyan/AT073_10_Orig_Style/Libs/Tensorfl...</td>\n",
       "      <td>37812209_310102473067178_1157119865203458048_n</td>\n",
       "      <td>json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44702348_184755972468726_7981390250275482789_n...</td>\n",
       "      <td>/home/jovyan/AT073_10_Orig_Style/Libs/Tensorfl...</td>\n",
       "      <td>44702348_184755972468726_7981390250275482789_n</td>\n",
       "      <td>json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37318778_419922151848914_7854674386068963328_n...</td>\n",
       "      <td>/home/jovyan/AT073_10_Orig_Style/Libs/Tensorfl...</td>\n",
       "      <td>37318778_419922151848914_7854674386068963328_n</td>\n",
       "      <td>json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29715768_2036105416656742_215227906488532992_n...</td>\n",
       "      <td>/home/jovyan/AT073_10_Orig_Style/Libs/Tensorfl...</td>\n",
       "      <td>29715768_2036105416656742_215227906488532992_n</td>\n",
       "      <td>json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  43766464_257490018303217_3555742597326072823_n...   \n",
       "4  37812209_310102473067178_1157119865203458048_n...   \n",
       "5  44702348_184755972468726_7981390250275482789_n...   \n",
       "6  37318778_419922151848914_7854674386068963328_n...   \n",
       "7  29715768_2036105416656742_215227906488532992_n...   \n",
       "\n",
       "                                                path  \\\n",
       "0  /home/jovyan/AT073_10_Orig_Style/Libs/Tensorfl...   \n",
       "4  /home/jovyan/AT073_10_Orig_Style/Libs/Tensorfl...   \n",
       "5  /home/jovyan/AT073_10_Orig_Style/Libs/Tensorfl...   \n",
       "6  /home/jovyan/AT073_10_Orig_Style/Libs/Tensorfl...   \n",
       "7  /home/jovyan/AT073_10_Orig_Style/Libs/Tensorfl...   \n",
       "\n",
       "                                             file doc_type  \n",
       "0  43766464_257490018303217_3555742597326072823_n     json  \n",
       "4  37812209_310102473067178_1157119865203458048_n     json  \n",
       "5  44702348_184755972468726_7981390250275482789_n     json  \n",
       "6  37318778_419922151848914_7854674386068963328_n     json  \n",
       "7  29715768_2036105416656742_215227906488532992_n     json  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata = pd.DataFrame({'name':os.listdir()})\n",
    "mydata['path'] = '/home/jovyan/AT073_10_Orig_Style/Libs/Tensorflow/models/data/deejaysoda-2018-12-08/'+ mydata['name']\n",
    "mydata['file'], mydata['doc_type'] = mydata['name'].str.split('.', 1).str\n",
    "train_data = mydata[mydata['doc_type'].str.lower().eq('json')]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 中有 5 個label\n",
      "Height = 1349\n",
      "Width = 1080\n",
      "image 中有 13 個label\n",
      "Height = 1080\n",
      "Width = 1080\n",
      "image 中有 3 個label\n",
      "Height = 1080\n",
      "Width = 1080\n",
      "image 中有 10 個label\n",
      "Height = 945\n",
      "Width = 945\n",
      "image 中有 4 個label\n",
      "Height = 853\n",
      "Width = 853\n",
      "image 中有 1 個label\n",
      "Height = 1271\n",
      "Width = 1080\n",
      "image 中有 11 個label\n",
      "Height = 953\n",
      "Width = 953\n",
      "image 中有 1 個label\n",
      "Height = 930\n",
      "Width = 930\n",
      "image 中有 1 個label\n",
      "Height = 1079\n",
      "Width = 1080\n",
      "image 中有 16 個label\n",
      "Height = 890\n",
      "Width = 890\n",
      "image 中有 8 個label\n",
      "Height = 1350\n",
      "Width = 1080\n",
      "image 中有 18 個label\n",
      "Height = 887\n",
      "Width = 887\n",
      "image 中有 2 個label\n",
      "Height = 1249\n",
      "Width = 1080\n",
      "image 中有 9 個label\n",
      "Height = 657\n",
      "Width = 657\n",
      "image 中有 5 個label\n",
      "Height = 1258\n",
      "Width = 1006\n",
      "image 中有 5 個label\n",
      "Height = 1079\n",
      "Width = 1080\n",
      "image 中有 2 個label\n",
      "Height = 858\n",
      "Width = 858\n",
      "image 中有 7 個label\n",
      "Height = 1079\n",
      "Width = 1080\n",
      "image 中有 5 個label\n",
      "Height = 851\n",
      "Width = 851\n",
      "image 中有 7 個label\n",
      "Height = 405\n",
      "Width = 719\n",
      "image 中有 2 個label\n",
      "Height = 655\n",
      "Width = 655\n",
      "image 中有 2 個label\n",
      "Height = 1080\n",
      "Width = 1080\n",
      "image 中有 4 個label\n",
      "Height = 1080\n",
      "Width = 1080\n",
      "image 中有 12 個label\n",
      "Height = 1349\n",
      "Width = 1080\n",
      "image 中有 1 個label\n",
      "Height = 1349\n",
      "Width = 1080\n",
      "image 中有 2 個label\n",
      "Height = 826\n",
      "Width = 826\n",
      "image 中有 8 個label\n",
      "Height = 1020\n",
      "Width = 1020\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for mask_png in train_data['file']:\n",
    "    json_data = open(mask_png + \".json\").read()\n",
    "    data = json.loads(json_data)\n",
    "\n",
    "    if len(data['shapes']) == 0:\n",
    "        print(\"image 中有\",len(data['shapes']),\"個label\")\n",
    "    else:\n",
    "        print(\"image 中有\",len(data['shapes']),\"個label\")       \n",
    "        data['shapes'][0]\n",
    "        Height = data['imageHeight']\n",
    "        Width = data['imageWidth']\n",
    "        print('Height =',data['imageHeight'])\n",
    "        print('Width =',data['imageWidth'])\n",
    "\n",
    "        person = []\n",
    "        background = []\n",
    "\n",
    "        for i in range(len(data['shapes'])):\n",
    "            if data['shapes'][i].get('label') == 'person':\n",
    "                person.append(np.array(data['shapes'][i]['points']).astype(int))\n",
    "            elif data['shapes'][i].get('label') == 'background':\n",
    "                background.append(np.array(data['shapes'][i]['points']).astype(int))\n",
    "            else:\n",
    "                print('other')\n",
    "\n",
    "        org_img = np.zeros((Height,Width),dtype=np.uint8)\n",
    "        for p in person:"
    "           mask = cv2.fillPoly(org_img, [p], 1) #先畫人\n",
    "        mask = cv2.fillPoly(mask, background, 0)  #再補空洞\n",
    "        mask = mask.astype(np.int64)\n",
    "        seg_image = label_to_color_image(mask).astype(np.uint8)\n",
    "        plt.imshow(seg_image)\n",
    "        img=PIL.Image.fromarray(seg_image)\n",
    "        img.save(mask_png + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
